# -*- coding: utf-8 -*-
"""Submission1MachineLearningIntermediate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J0ZJFULOhNDZUUJ1uuLIBz-fsfDKlmtK

#Import dan Download Semua Package
"""

import os
import pandas as pd

import nltk, os, re, string

from keras.layers import Input, LSTM, Bidirectional, SpatialDropout1D, Dropout, Flatten, Dense, Embedding, BatchNormalization
from keras.models import Model
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences


from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import matplotlib.pyplot as plt

"""#Fetch Dataset dari Kaggle"""

os.environ['KAGGLE_CONFIG_DIR'] = '/content'
!kaggle datasets download -d gpreda/covid-world-vaccination-progress

df = pd.read_csv('country_vaccinations.csv')
df.head(10)

"""#Daftar Nama Kolom"""

df.columns

df.shape

df.info()

"""#Daftar Negara"""

df.country.value_counts()

"""#Daftar Vaksin"""

df.vaccines.value_counts()

df_new = df.drop(columns=[
                          'iso_code', 'total_vaccinations', 'people_vaccinated', 
                          'people_fully_vaccinated', 'daily_vaccinations_raw', 
                          'daily_vaccinations', 'total_vaccinations_per_hundred', 
                          'people_vaccinated_per_hundred', 
                          'people_fully_vaccinated_per_hundred', 
                          'daily_vaccinations_per_million'
                          ])
df_new

"""#Download wordnet dan stopwords"""

nltk.download('wordnet')
nltk.download('stopwords')

"""#Lower-case all characters"""

df_new.country = df_new.country.apply(lambda x: x.lower())
df_new.date = df_new.date.apply(lambda x: x.lower())
df_new.vaccines = df_new.vaccines.apply(lambda x: x.lower())
df_new.source_name = df_new.source_name.apply(lambda x: x.lower())
df_new.source_website = df_new.source_website.apply(lambda x: x.lower())

"""#Menghapus functuation"""

def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    df_new.country = df_new.country.apply(lambda x: cleaner(x))
    df_new.date = df_new.date.apply(lambda x: lem(x))
    df_new.vaccines = df_new.vaccines.apply(lambda x: lem(x))
    df_new.source_name = df_new.source_name.apply(lambda x: lem(x))
    df_new.source_website = df_new.source_website.apply(lambda x: lem(x))

"""#Lematization"""

lemmatizer = WordNetLemmatizer()
def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    df_new.country = df_new.country.apply(lambda x: lem(x))
    df_new.date = df_new.date.apply(lambda x: lem(x))
    df_new.vaccines = df_new.vaccines.apply(lambda x: lem(x))
    df_new.source_name = df_new.source_name.apply(lambda x: lem(x))
    df_new.source_website = df_new.source_website.apply(lambda x: lem(x))

"""#Menghapus Angka"""

def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    df_new['country'].apply(rem_numbers)
    df_new['date'].apply(rem_numbers)
    df_new['vaccines'].apply(rem_numbers)
    df_new['source_name'].apply(rem_numbers)
    df_new['source_website'].apply(rem_numbers)

"""#Menghapus Stopword"""

st_words = stopwords.words()
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    df_new.country = df_new.country.apply(lambda x: stopword(x))
    df_new.date = df_new.date.apply(lambda x: lem(x))
    df_new.vaccines = df_new.vaccines.apply(lambda x: lem(x))
    df_new.source_name = df_new.source_name.apply(lambda x: lem(x))
    df_new.source_website = df_new.source_website.apply(lambda x: lem(x))

"""#Menampilkan Data Setelah Cleansing"""

df_new.head(10)

category = pd.get_dummies(df_new.vaccines)
df_new_cat = pd.concat([df_new, category], axis=1)
df_new_cat = df_new_cat.drop(columns='vaccines')
df_new_cat.head(20)

"""#Inisiasi Variabel untuk membuat Model Fit"""

covid = df_new_cat['country'].values + '' + df_new_cat['date'].values + '' + df_new_cat['source_name'].values  + '' + df_new_cat['source_website'].values
vaksin = df_new_cat[['abdala, soberana02', 'cansino, oxford/astrazeneca, pfizer/biontech, sinovac', 'cansino, sinopharm/beijing, sinopharm/wuhan, sinovac', 'covaxin, oxford/astrazeneca', 'covaxin, oxford/astrazeneca, sinopharm/beijing', 'covaxin, oxford/astrazeneca, sinopharm/beijing, sputnik v', 'covaxin, oxford/astrazeneca, sputnik v', 'johnson&johnson, moderna, oxford/astrazeneca, pfizer/biontech', 'johnson&johnson, moderna, oxford/astrazeneca, pfizer/biontech, sinopharm/beijing, sputnik v', 'johnson&johnson, moderna, oxford/astrazeneca, pfizer/biontech, sinovac', 'johnson&johnson, moderna, oxford/astrazeneca, pfizer/biontech, sputnik v', 'johnson&johnson, moderna, pfizer/biontech', 'johnson&johnson, oxford/astrazeneca', 'johnson&johnson, oxford/astrazeneca, pfizer/biontech, sinopharm/beijing', 'johnson&johnson, oxford/astrazeneca, pfizer/biontech, sinovac', 'johnson&johnson, oxford/astrazeneca, sinopharm/beijing', 'johnson&johnson, oxford/astrazeneca, sinopharm/beijing, sinovac', 'johnson&johnson, oxford/astrazeneca, sinopharm/beijing, sinovac, sputnik v', 'johnson&johnson, pfizer/biontech', 'moderna', 'moderna, oxford/astrazeneca', 'moderna, oxford/astrazeneca, pfizer/biontech', 'moderna, oxford/astrazeneca, pfizer/biontech, sinopharm/beijing', 'moderna, oxford/astrazeneca, sinopharm/beijing, sinovac', 'moderna, oxford/astrazeneca, sinopharm/beijing, sputnik v', 'moderna, pfizer/biontech', 'oxford/astrazeneca', 'oxford/astrazeneca, pfizer/biontech', 'oxford/astrazeneca, pfizer/biontech, sinopharm/beijing, sinovac', 'oxford/astrazeneca, pfizer/biontech, sinopharm/beijing, sinovac, sputnik v', 'oxford/astrazeneca, pfizer/biontech, sinopharm/beijing, sputnik v', 'oxford/astrazeneca, pfizer/biontech, sinovac', 'oxford/astrazeneca, pfizer/biontech, sinovac, sputnik v', 'oxford/astrazeneca, sinopharm/beijing', 'oxford/astrazeneca, sinopharm/beijing, sinovac, sputnik v', 'oxford/astrazeneca, sinovac', 'oxford/astrazeneca, sinovac, sputnik v', 'oxford/astrazeneca, sputnik v', 'pfizer/biontech', 'pfizer/biontech, sinovac', 'sinopharm/beijing', 'sinopharm/beijing, sputnik v', 'sputnik v']].values

"""#Menampilkan Himpunan Covid"""

covid

"""#Menampilkan Himpunan Vaksin"""

vaksin

"""#Split"""

covid_train, covid_test, vaksin_train, vaksin_test = train_test_split(covid, vaksin, test_size=0.2, shuffle=True)

"""# Tokenizer"""

tokenizer = Tokenizer(num_words=5000, oov_token='x', filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')
tokenizer.fit_on_texts(covid_train) 
tokenizer.fit_on_texts(covid_test)
 
sekuens_train = tokenizer.texts_to_sequences(covid_train)
sekuens_test = tokenizer.texts_to_sequences(covid_test)
 
padded_train = pad_sequences(sekuens_train) 
padded_test = pad_sequences(sekuens_test)

"""#Membangun Model menggunakan Model Sequential"""

# model
import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(43, activation='softmax')
])
model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

"""#Implementasi Callback Apabila memenuhi Standar Akurasi yang Sudah Ditentukan"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92 and logs.get('val_accuracy')>0.92):
      self.model.stop_training = True
      print("\nAkurasi pada training set dan validation set di atas 90%")
callbacks = myCallback()

"""#Pelatihan Model"""

history = model.fit(padded_train, vaksin_train, epochs=50, 
                    validation_data=(padded_test, vaksin_test), verbose=2, callbacks=[callbacks], validation_steps=30)

"""#Grafik Akurasi"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Grafik Akurasi')
plt.ylabel('Nilai Akurasi')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#Grafik Loss"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Grafik Loss')
plt.ylabel('Nilai Loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()